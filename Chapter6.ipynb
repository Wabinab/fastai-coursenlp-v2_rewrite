{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Chapter 6: RNN English Numbers\n",
    "## Predicting English word version of numbers using RNN\n",
    "\n",
    "There is an english language corpus which list the numbers in orders: \n",
    "\n",
    "eight thousand one, eight thousand two...\n",
    "\n",
    "created by Jeremy to check if things are working, to debug, and understand what was going on. When experimenting with new ideas, it's nice to have smaller dataset to do so, quickly get a sense of whether your ideas are promising (Imagenette and Imagewoof for computer visual). English word numbers serve as a good dataset to learn about RNNs. \n",
    "\n",
    "IN DL: there are 2 types of numbers: \n",
    "\n",
    "- **Parameters**: numbers that are learned. \n",
    "- **Activations**: numbers that are calculated (by affine functions & element-wise non-linearities). \n",
    "\n",
    "When learning new concept in DL, ask yourself: *Is this a parameter or an activation?*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from fastai.text.all import *\n",
    "bs = 64\n",
    "\n",
    "path = untar_data(URLs.HUMAN_NUMBERS)\n",
    "path.ls()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(#2) [Path('/home/fastai2/.fastai/data/human_numbers/valid.txt'),Path('/home/fastai2/.fastai/data/human_numbers/train.txt')]"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def readnums(d): return[\", \".join(o.strip() for o in open(path/d).readlines())]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "train_txt = readnums(\"train.txt\")\n",
    "train_txt[0][:80]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'one, two, three, four, five, six, seven, eight, nine, ten, eleven, twelve, thirt'"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "valid_txt = readnums(\"valid.txt\")\n",
    "valid_txt[0][-80:]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "' nine thousand nine hundred ninety eight, nine thousand nine hundred ninety nine'"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "From here onwards it has been difficult to direct translate what is being done. See fastbook Chapter 12 for the new version of RNN introduction. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "lines = L()\n",
    "with open(path/\"train.txt\") as f: lines += L(*f.readlines())\n",
    "with open(path/\"valid.txt\") as f: lines += L(*f.readlines())\n",
    "lines"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(#9998) ['one \\n','two \\n','three \\n','four \\n','five \\n','six \\n','seven \\n','eight \\n','nine \\n','ten \\n'...]"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "text = \" . \".join([l.strip() for l in lines]) # separator\n",
    "text[:100]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'one . two . three . four . five . six . seven . eight . nine . ten . eleven . twelve . thirteen . fo'"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "tokens = text.split(\" \")\n",
    "vocab = L(*tokens).unique()\n",
    "vocab"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(#30) ['one','.','two','three','four','five','six','seven','eight','nine'...]"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "word2idx = {w:i for i, w in enumerate(vocab)}\n",
    "nums = L(word2idx[i] for i in tokens)\n",
    "nums"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(#63095) [0,1,2,1,3,1,4,1,5,1...]"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "seqs = L((tensor(nums[i : i + 3]), nums[i + 3]) for i in range(0, len(nums) - 4, 3))\n",
    "seqs"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(#21031) [(tensor([0, 1, 2]), 1),(tensor([1, 3, 1]), 4),(tensor([4, 1, 5]), 1),(tensor([1, 6, 1]), 7),(tensor([7, 1, 8]), 1),(tensor([1, 9, 1]), 10),(tensor([10,  1, 11]), 1),(tensor([ 1, 12,  1]), 13),(tensor([13,  1, 14]), 1),(tensor([ 1, 15,  1]), 16)...]"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "cut = int(len(seqs) * 0.8)\n",
    "dls = DataLoaders.from_dsets(seqs[:cut], seqs[cut:], bs=bs, shuffle=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that even if we do this, we also have a different value. This is because how we split. The original one is a long line of str, this splits three by three. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "dls.valid_ds[0][0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([ 1,  8, 29])"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "One doesn't know how to get the bptt nor valid_dl. Most importantly, we can't even `dls.show_batch()`. However, we will skip directly to the model part. \n",
    "\n",
    "- i_h: input to hidden. \n",
    "- h_h: hidden to hidden\n",
    "- h_o: hidden to output\n",
    "- bn: batchnorm"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "class Model0(Module):\n",
    "    def __init__(self, vocab_sz, n_hidden):\n",
    "        # we don't need to super().__init__() anymore with newer version\n",
    "        # of PyTorch. \n",
    "        nv, nh = vocab_sz, n_hidden\n",
    "        self.i_h = nn.Embedding(nv, nh)\n",
    "        self.h_h = nn.Linear(nh, nh)\n",
    "        self.h_o = nn.Linear(nh, nv)\n",
    "        self.bn = nn.BatchNorm1d(nh)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.bn(F.relu(self.i_h(x[:, 0])))\n",
    "        if x.shape[1] > 1: \n",
    "            h += self.i_h(x[:, 1])\n",
    "            h = self.bn(F.relu(self.h_h(h)))\n",
    "        if x.shape[1] > 2:\n",
    "            h += self.i_h(x[:, 2])\n",
    "            h = self.bn(F.relu(self.h_h(h)))\n",
    "        return self.h_o(h)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "learn = Learner(dls, Model0(len(vocab), 64), loss_func=F.cross_entropy, \n",
    "            metrics=accuracy)\n",
    "learn.fit_one_cycle(6, 1e-4)"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.308321</td>\n",
       "      <td>3.394462</td>\n",
       "      <td>0.045876</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.672316</td>\n",
       "      <td>2.868347</td>\n",
       "      <td>0.335631</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.158960</td>\n",
       "      <td>2.460403</td>\n",
       "      <td>0.403375</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.917717</td>\n",
       "      <td>2.269640</td>\n",
       "      <td>0.437842</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.825647</td>\n",
       "      <td>2.206414</td>\n",
       "      <td>0.441169</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.803799</td>\n",
       "      <td>2.197597</td>\n",
       "      <td>0.440694</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "For how to use this to do predictions, or how good it is, check out Chapter 12 of fastbook. \n",
    "\n",
    "## Same thing with a loop"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "class Model1(Module):\n",
    "    def __init__(self, vocab_sz, n_hidden):\n",
    "        # we don't need to super().__init__() anymore with newer version\n",
    "        # of PyTorch. \n",
    "        nv, nh = vocab_sz, n_hidden\n",
    "        self.i_h = nn.Embedding(nv, nh)\n",
    "        self.h_h = nn.Linear(nh, nh)\n",
    "        self.h_o = nn.Linear(nh, nv)\n",
    "        self.bn = nn.BatchNorm1d(nh)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.bn(F.relu(self.i_h(x[:, 0])))\n",
    "        for i in range(x.shape[1]):\n",
    "            h += self.i_h(x[:, i])\n",
    "            h = self.bn(F.relu(self.h_h(h)))\n",
    "        return self.h_o(h)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "def fit_model(model, epochs, lr, **kwargs):\n",
    "    learn = Learner(dls, model, loss_func=F.cross_entropy, \n",
    "                metrics=accuracy)\n",
    "    learn.fit_one_cycle(epochs, lr, **kwargs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "fit_model(Model1(len(vocab), 64), 6, 1e-4)"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.391133</td>\n",
       "      <td>3.453091</td>\n",
       "      <td>0.036843</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.796483</td>\n",
       "      <td>2.979537</td>\n",
       "      <td>0.249584</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.320133</td>\n",
       "      <td>2.666550</td>\n",
       "      <td>0.325648</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.063687</td>\n",
       "      <td>2.512197</td>\n",
       "      <td>0.340385</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.958931</td>\n",
       "      <td>2.457953</td>\n",
       "      <td>0.352270</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.933541</td>\n",
       "      <td>2.450973</td>\n",
       "      <td>0.365343</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Multi fully connected model\n",
    "Why not predict token 2 from token 1, token 3 from token 2, etc? One doesn't know how to make modifications since our bptt isn't defined in new fastai. Perhaps it's sequence length so let's try that. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "m = len(seqs) // bs\n",
    "m, bs, len(seqs)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(328, 64, 21031)"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "def group_chunks(ds, bs): \n",
    "    m = len(ds) // bs\n",
    "    new_ds = L()\n",
    "    for i in range(m): new_ds += L(ds[i + m * j] for j in range(bs))\n",
    "    return new_ds"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "sl = 16  # better than bptt = sl = 20\n",
    "seqs = L((tensor(nums[i : i + sl]), tensor(nums[i + 1: i + sl + 1]))\n",
    "        for i in range(0, len(nums) - sl - 1, sl))\n",
    "cut = int(len(seqs) * 0.8)\n",
    "dls = DataLoaders.from_dsets(group_chunks(seqs[:cut], bs),\n",
    "                             group_chunks(seqs[cut:], bs),\n",
    "                             bs=bs, drop_last=True, shuffle=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "class Model2(Module):\n",
    "    def __init__(self, nv, nh):\n",
    "        self.nh = nh\n",
    "        self.i_h = nn.Embedding(nv, nh)\n",
    "        self.h_h = nn.Linear(nh, nh)\n",
    "        self.h_o = nn.Linear(nh, nv)\n",
    "        self.bn = nn.BatchNorm1d(nh)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = torch.zeros(x.shape[0], self.nh).to(device=x.device)\n",
    "        # h = 0\n",
    "        res = []\n",
    "        for i in range(x.shape[1]):\n",
    "            h = h + self.i_h(x[:, i])\n",
    "            h = F.relu(self.h_h(h))\n",
    "            res.append(self.h_o(self.bn(h)))\n",
    "        # self.h = self.h.detach()\n",
    "        return torch.stack(res, dim=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "def loss_func(inp, targ):\n",
    "    return F.cross_entropy(inp.view(-1, len(vocab)), targ.view(-1))\n",
    "\n",
    "def fit_model(model, epochs, lr, **kwargs):\n",
    "    learn = Learner(dls, model, loss_func=loss_func, \n",
    "                metrics=accuracy)\n",
    "    learn.fit_one_cycle(epochs, lr, **kwargs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "fit_model(Model2(len(vocab), 64), 10, 1e-4, pct_start=0.1)"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.613575</td>\n",
       "      <td>3.539885</td>\n",
       "      <td>0.030518</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.402714</td>\n",
       "      <td>3.313725</td>\n",
       "      <td>0.061442</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.147016</td>\n",
       "      <td>3.100097</td>\n",
       "      <td>0.173340</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.898019</td>\n",
       "      <td>2.918723</td>\n",
       "      <td>0.261475</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.685772</td>\n",
       "      <td>2.779171</td>\n",
       "      <td>0.277018</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.521361</td>\n",
       "      <td>2.680985</td>\n",
       "      <td>0.288086</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.405792</td>\n",
       "      <td>2.620372</td>\n",
       "      <td>0.293132</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.332363</td>\n",
       "      <td>2.588161</td>\n",
       "      <td>0.294027</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.291258</td>\n",
       "      <td>2.575815</td>\n",
       "      <td>0.295085</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.272216</td>\n",
       "      <td>2.573973</td>\n",
       "      <td>0.295329</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you get `torch.size() not equal` something like that to the input it forces you to use vocab length, it's because of the loss function we previously defined not working anymore. Here, we define a new loss function to make it work. \n",
    "\n",
    "## Maintain State\n",
    "Keep hidden state from previous line of text, so we're not starting over again on each line of text. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "class Model3(Module):\n",
    "    def __init__(self, nv, nh):\n",
    "        self.i_h = nn.Embedding(nv, nh)\n",
    "        self.h_h = nn.Linear(nh, nh)\n",
    "        self.h_o = nn.Linear(nh, nv)\n",
    "        self.bn = nn.BatchNorm1d(nh)\n",
    "        self.h = torch.zeros(bs, nh)  # somehow cannot put on cuda\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = []\n",
    "        h = self.h\n",
    "        for i in range(x.shape[1]):\n",
    "            h = h + self.i_h(x[:, i])\n",
    "            h = F.relu(self.h_h(h))\n",
    "            res.append(self.bn(h))\n",
    "        self.h = h.detach()\n",
    "        res = torch.stack(res, dim=1)\n",
    "        res = self.h_o(res)\n",
    "        return res"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "fit_model(Model3(len(vocab), 64), 20, 3e-3)"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.268880</td>\n",
       "      <td>3.152752</td>\n",
       "      <td>0.177409</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.537653</td>\n",
       "      <td>2.118135</td>\n",
       "      <td>0.461914</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.908176</td>\n",
       "      <td>1.932215</td>\n",
       "      <td>0.376546</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.629920</td>\n",
       "      <td>1.963621</td>\n",
       "      <td>0.323730</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.515565</td>\n",
       "      <td>2.008920</td>\n",
       "      <td>0.319987</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.433965</td>\n",
       "      <td>1.786368</td>\n",
       "      <td>0.469727</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.295307</td>\n",
       "      <td>1.931554</td>\n",
       "      <td>0.478190</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.154681</td>\n",
       "      <td>1.888782</td>\n",
       "      <td>0.504232</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.044125</td>\n",
       "      <td>1.784899</td>\n",
       "      <td>0.531331</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.951643</td>\n",
       "      <td>1.705102</td>\n",
       "      <td>0.549805</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.865996</td>\n",
       "      <td>1.674796</td>\n",
       "      <td>0.554036</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.795238</td>\n",
       "      <td>1.654988</td>\n",
       "      <td>0.564860</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.735575</td>\n",
       "      <td>1.630730</td>\n",
       "      <td>0.579508</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.686574</td>\n",
       "      <td>1.606245</td>\n",
       "      <td>0.589193</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.647499</td>\n",
       "      <td>1.592392</td>\n",
       "      <td>0.595215</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.618740</td>\n",
       "      <td>1.609472</td>\n",
       "      <td>0.601888</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.598464</td>\n",
       "      <td>1.602034</td>\n",
       "      <td>0.597656</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.581827</td>\n",
       "      <td>1.605636</td>\n",
       "      <td>0.602132</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.572212</td>\n",
       "      <td>1.604656</td>\n",
       "      <td>0.605469</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.567325</td>\n",
       "      <td>1.601025</td>\n",
       "      <td>0.605143</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The previous cell we cannot put on cuda but don't worry we still get really fast calculation with 6 vCPUs. \n",
    "\n",
    "The reason is we need to put **ALL** the layers onto cuda as well, then we put data onto cuda as well. Perhaps you could call the \"object\" to cuda, one isn't sure. For this way, we demo how to put cuda *redundantly* on the next cell. \n",
    "\n",
    "## nn.RNN\n",
    "Refactor to use PyTorch's RNN. That's what we would use in practice, but now we know the inside details. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "class Model4(Module):\n",
    "    def __init__(self, nv, nh):\n",
    "        self.i_h = nn.Embedding(nv, nh).cuda()\n",
    "        self.rnn = nn.RNN(nh, nh, batch_first=True).cuda()\n",
    "        self.h_o = nn.Linear(nh, nv).cuda()\n",
    "        self.bn = BatchNorm1dFlat(nh).cuda()\n",
    "        self.h = torch.zeros(1, bs, nh).cuda()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.cuda()\n",
    "        res, h = self.rnn(self.i_h(x), self.h)\n",
    "        self.h = h.detach()\n",
    "        return self.h_o(self.bn(res))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "fit_model(Model4(len(vocab), 64), 20, 3e-3)"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.051171</td>\n",
       "      <td>2.653143</td>\n",
       "      <td>0.406494</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.254001</td>\n",
       "      <td>1.929059</td>\n",
       "      <td>0.471191</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.772074</td>\n",
       "      <td>1.903632</td>\n",
       "      <td>0.320882</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.575565</td>\n",
       "      <td>1.955284</td>\n",
       "      <td>0.321859</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.476673</td>\n",
       "      <td>1.901512</td>\n",
       "      <td>0.400065</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.268954</td>\n",
       "      <td>1.798347</td>\n",
       "      <td>0.396566</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.056735</td>\n",
       "      <td>1.577960</td>\n",
       "      <td>0.478353</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.876947</td>\n",
       "      <td>1.504796</td>\n",
       "      <td>0.532145</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.728600</td>\n",
       "      <td>1.509092</td>\n",
       "      <td>0.553141</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.602096</td>\n",
       "      <td>1.614856</td>\n",
       "      <td>0.560872</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.502838</td>\n",
       "      <td>1.624375</td>\n",
       "      <td>0.581868</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.429984</td>\n",
       "      <td>1.642909</td>\n",
       "      <td>0.600586</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.378050</td>\n",
       "      <td>1.632398</td>\n",
       "      <td>0.617676</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.340457</td>\n",
       "      <td>1.629626</td>\n",
       "      <td>0.623942</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.310329</td>\n",
       "      <td>1.592164</td>\n",
       "      <td>0.628255</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.286682</td>\n",
       "      <td>1.635798</td>\n",
       "      <td>0.627523</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.270177</td>\n",
       "      <td>1.661117</td>\n",
       "      <td>0.627930</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.256903</td>\n",
       "      <td>1.656633</td>\n",
       "      <td>0.628743</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.247870</td>\n",
       "      <td>1.671873</td>\n",
       "      <td>0.626628</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.242617</td>\n",
       "      <td>1.695050</td>\n",
       "      <td>0.623779</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2-layer GRU\n",
    "When we have long time scales and deeper networks, these become impossible to train. One way to address this is to add mini-NN to decide how much \"green arrow and orange arrow\" (see slides in github repo) to keep. These mini-NNs can be GRUs or LSTMs. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "class Model5(Module):\n",
    "    def __init__(self, nv, nh):\n",
    "        self.i_h = nn.Embedding(nv, nh).cuda()\n",
    "        self.rnn = nn.GRU(nh, nh, 2, batch_first=True).cuda()\n",
    "        self.h_o = nn.Linear(nh, nv).cuda()\n",
    "        self.bn = BatchNorm1dFlat(nh).cuda()\n",
    "        self.h = torch.zeros(2, bs, nh).cuda()\n",
    "\n",
    "    def forward(self, x):\n",
    "        res, h = self.rnn(self.i_h(x), self.h)\n",
    "        self.h = h.detach()\n",
    "        return self.h_o(self.bn(res))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "fit_model(Model5(len(vocab), 64), 10, 1e-2)"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.229088</td>\n",
       "      <td>1.741237</td>\n",
       "      <td>0.520752</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.071178</td>\n",
       "      <td>1.000395</td>\n",
       "      <td>0.759928</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.444417</td>\n",
       "      <td>0.752374</td>\n",
       "      <td>0.825358</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.195652</td>\n",
       "      <td>0.793777</td>\n",
       "      <td>0.829671</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.092799</td>\n",
       "      <td>0.872465</td>\n",
       "      <td>0.814290</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.048371</td>\n",
       "      <td>0.934903</td>\n",
       "      <td>0.821045</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.028002</td>\n",
       "      <td>0.931070</td>\n",
       "      <td>0.828613</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.017872</td>\n",
       "      <td>0.973533</td>\n",
       "      <td>0.816406</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.011911</td>\n",
       "      <td>1.010683</td>\n",
       "      <td>0.818604</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.008604</td>\n",
       "      <td>1.022495</td>\n",
       "      <td>0.815348</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ignoring the above overfitting model. \n",
    "\n",
    "ULMFiT: we swap out `self.h_o` with a classifier to do classification on text. \n",
    "\n",
    "RNNs are just refactored, fully-connected NNs. We can use same approach for any sequence labeling task (part of speech, classifying whether material is sensitive, etc...)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.2",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit ('fastai': conda)"
  },
  "interpreter": {
   "hash": "5aa457f694240ca52bec53eda6ed84b45efde55787bbd717cb0e138c3a892911"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}